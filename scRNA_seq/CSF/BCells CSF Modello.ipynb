{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549a5a4e-7c17-48ed-b38b-4a763a9c7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling.base import BaseOverSampler\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, classification_report\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import math\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "\n",
    "# CSF EDITS\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1facdbbb-d40a-444c-91d0-fd2cbf9f404e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/store/France_CSF/B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8ee821-8e11-453a-b21c-caf3ad29b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"bcells_csf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3429afd-3885-4370-854b-3b8fbf0617db",
   "metadata": {},
   "source": [
    "<h2>Preparazione dei dati</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a24f19-247c-4f19-8d49-9d46855d77e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2143, 17820)\n"
     ]
    }
   ],
   "source": [
    "dataset = sc.read_h5ad(f\"{dataset_name}.h5ad\")\n",
    "print(dataset.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f147aa6-1447-40f8-a8b6-c3dc4d515f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleID = dataset.obs['sample']\n",
    "indicator = dataset.obs['condition']\n",
    "datasetID = dataset.obs['dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f5400-c300-4f3c-ab63-7b81029c16cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2>Analisi di correlazione</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f5a8aa-8def-43b6-bb8c-4509cd2e162d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bcells_csf_dataCorr.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current = dataset.copy()\n",
    "data = pd.DataFrame(current.X, columns=current.var_names)\n",
    "dataCorr = data.corr()\n",
    "\n",
    "joblib.dump(dataCorr, f'{dataset_name}_dataCorr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f8df49-1978-4037-897a-e5be12855c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bcells_csf_clusters.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCorr = joblib.load(f'{dataset_name}_dataCorr.pkl')\n",
    "dataCorr.index = dataset.var_names\n",
    "dataCorr.columns = dataset.var_names\n",
    "\n",
    "abs_corr = dataCorr.abs()\n",
    "\n",
    "clusters = []\n",
    "\n",
    "for gene_i in dataCorr.index:\n",
    "    # Get all correlations >= 0.9 for this gene\n",
    "    high_corr_mask = abs_corr.loc[gene_i] >= 0.9\n",
    "    cluster = dataCorr.columns[high_corr_mask].tolist()\n",
    "    \n",
    "    if len(cluster) > 1:  # Only add if cluster has more than 1 gene\n",
    "        clusters.append(cluster)\n",
    "\n",
    "joblib.dump(clusters, f\"{dataset_name}_clusters.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff41274-d8b5-41ce-8ff5-b96a61b88b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = joblib.load(f\"{dataset_name}_clusters.pkl\")\n",
    "\n",
    "gene_list = list(dataset.var_names)\n",
    "\n",
    "for cluster in clusters:\n",
    "    for gene in cluster:\n",
    "        if gene in gene_list:\n",
    "            gene_list.remove(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ea2440-c385-43bf-a615-484060c183a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = dataset.copy()\n",
    "data = pd.DataFrame(current.X, columns=current.var_names)\n",
    "\n",
    "# Calculate variances\n",
    "all_variances = {var: float(data[var].var()) for var in data.columns}\n",
    "\n",
    "# Sort all variables by variance (highest to lowest) once\n",
    "sorted_vars = sorted(all_variances.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_vars_list = [var for var, _ in sorted_vars]\n",
    "\n",
    "representative = {}\n",
    "# Process clusters by size (largest first)\n",
    "for cluster in sorted(clusters, key=len, reverse=True):\n",
    "    cluster_tuple = tuple(cluster)\n",
    "\n",
    "    # Find the highest variance variable in this cluster that isn't already used\n",
    "    found_representative = False\n",
    "    for var in sorted_vars_list:\n",
    "        if var in cluster and var not in representative.values():\n",
    "            representative[cluster_tuple] = var\n",
    "            found_representative = True\n",
    "            break\n",
    "\n",
    "    # If we couldn't find an unused representative, use the highest variance one anyway\n",
    "    if not found_representative:\n",
    "        for var in sorted_vars_list:\n",
    "            if var in cluster:\n",
    "                representative[cluster_tuple] = var\n",
    "                break\n",
    "\n",
    "# Filter out subsets more efficiently\n",
    "unique_clusters = {}\n",
    "cluster_tuples = list(representative.keys())\n",
    "\n",
    "# Sort by length once\n",
    "cluster_tuples.sort(key=len)\n",
    "\n",
    "# Use a more efficient algorithm to filter subsets\n",
    "for i, cluster in enumerate(cluster_tuples):\n",
    "    is_unique = True\n",
    "    cluster_set = set(cluster)\n",
    "\n",
    "    # Only check against larger clusters\n",
    "    for j in range(i + 1, len(cluster_tuples)):\n",
    "        larger_cluster = cluster_tuples[j]\n",
    "        if cluster_set.issubset(set(larger_cluster)):\n",
    "            is_unique = False\n",
    "            break\n",
    "\n",
    "    if is_unique:\n",
    "        unique_clusters[cluster] = representative[cluster]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a9d87f-56a5-4924-bd01-9c71b73ae0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15651\n"
     ]
    }
   ],
   "source": [
    "gene_list.extend(gene for gene in unique_clusters.values() if gene not in gene_list)\n",
    "\n",
    "print(len(gene_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be38ae4b-00a7-40f7-a191-4a2c66bfbe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 2143 × 15651\n",
       "    obs: 'condition', 'sample', 'tissue', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_genes', 'n_counts', 'predicted_labels', 'over_clustering', 'majority_voting', 'dataset', 'generalized_celltype', '_scvi_batch', '_scvi_labels', 'concat_batch'\n",
       "    uns: '_scvi_manager_uuid', '_scvi_uuid', 'dataset_colors', 'generalized_celltype_colors', 'majority_voting_colors', 'neighbors', 'pca', 'sample_colors', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'corrected_latent', 'latent'\n",
       "    varm: 'PCs'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetDeclustered = dataset[:, gene_list]\n",
    "datasetDeclustered.write_h5ad(f\"{dataset_name}_datasetDeclustered.h5ad\")\n",
    "datasetDeclustered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b58f8f-2205-4751-9a45-cb0577c3a480",
   "metadata": {},
   "source": [
    "<h2>Divisione in train e test</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90af5f7f-fb3f-45af-85a8-d9b9cc307d79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miglior random_state: 523\n",
      "Statistiche: {'train_size': 1687, 'test_size': 456, 'train_ms': 927, 'train_ctrl': 760, 'train_ratio': 1.2197368421052632, 'test_ms': 228, 'test_ctrl': 228, 'test_ratio': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_best_random_state(dataset, n_trials=1000, target_train_size=0.75, target_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Trova il miglior random_state per GroupShuffleSplit che:\n",
    "    1. Si avvicina a una divisione 80% train, 20% test\n",
    "    2. Ha il rapporto più bilanciato possibile (vicino a 1:1) tra le condizioni nel train e nel test\n",
    "    \n",
    "    Args:\n",
    "        dataset: AnnData object con i dati e le annotazioni\n",
    "        n_trials: numero di random_state da provare\n",
    "        target_train_size: dimensione desiderata per il train set (default 0.8)\n",
    "        target_ratio: rapporto desiderato tra le condizioni (default 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Il miglior random_state trovato e le statistiche corrispondenti\n",
    "    \"\"\"\n",
    "    best_score = float('inf')\n",
    "    best_random_state = None\n",
    "    best_stats = None\n",
    "    \n",
    "    # Conta il numero totale di campioni per condizione\n",
    "    total_ms = np.sum(dataset.obs['condition'] == 'MS')\n",
    "    total_ctrl = np.sum(dataset.obs['condition'] == 'CTRL')\n",
    "    total_samples = len(dataset.obs)\n",
    "    \n",
    "    for random_state in range(n_trials):\n",
    "        # Esegui la divisione\n",
    "        splitter = GroupShuffleSplit(n_splits=2, test_size=1-target_train_size, random_state=random_state)\n",
    "        split = splitter.split(dataset.X, groups=dataset.obs['sample'])\n",
    "        train_inds, test_inds = next(split)\n",
    "        \n",
    "        # Crea i subset\n",
    "        train = dataset[train_inds].copy()\n",
    "        test = dataset[test_inds].copy()\n",
    "        \n",
    "        # Calcola le statistiche\n",
    "        train_ms = np.sum(train.obs['condition'] == 'MS')\n",
    "        train_ctrl = np.sum(train.obs['condition'] == 'CTRL')\n",
    "        test_ms = np.sum(test.obs['condition'] == 'MS')\n",
    "        test_ctrl = np.sum(test.obs['condition'] == 'CTRL')\n",
    "        \n",
    "        # Calcola i rapporti\n",
    "        train_ratio = train_ms / train_ctrl if train_ctrl != 0 else float('inf')\n",
    "        test_ratio = test_ms / test_ctrl if test_ctrl != 0 else float('inf')\n",
    "        \n",
    "        # Calcola gli scostamenti dal target\n",
    "        train_size_diff = abs(len(train_inds)/total_samples - target_train_size)\n",
    "        train_ratio_diff = abs(train_ratio - target_ratio)\n",
    "        test_ratio_diff = abs(test_ratio - target_ratio)\n",
    "        \n",
    "        # Ponderazione degli scostamenti (puoi modificare questi pesi in base alle tue priorità)\n",
    "        score = 0.3 * train_ratio_diff + 0.7 * test_ratio_diff + train_size_diff\n",
    "        \n",
    "        # Aggiorna il miglior risultato\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_random_state = random_state\n",
    "            best_stats = {\n",
    "                'train_size': len(train_inds),\n",
    "                'test_size': len(test_inds),\n",
    "                'train_ms': train_ms,\n",
    "                'train_ctrl': train_ctrl,\n",
    "                'train_ratio': train_ratio,\n",
    "                'test_ms': test_ms,\n",
    "                'test_ctrl': test_ctrl,\n",
    "                'test_ratio': test_ratio\n",
    "            }\n",
    "    \n",
    "    return best_random_state, best_stats\n",
    "\n",
    "\n",
    "best_rs, stats = find_best_random_state(dataset)\n",
    "print(f\"Miglior random_state: {best_rs}\")\n",
    "print(f\"Statistiche: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54537175-afa6-4f7a-badd-330f4a01ca18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('19270', 'MS'): 217,\n",
       " ('32190', 'CTRL'): 13,\n",
       " ('41540', 'CTRL'): 177,\n",
       " ('45044', 'CTRL'): 570,\n",
       " ('49131', 'MS'): 80,\n",
       " ('58637', 'MS'): 40,\n",
       " ('60249', 'MS'): 493,\n",
       " ('71658', 'MS'): 78,\n",
       " ('74594', 'MS'): 12,\n",
       " ('83775', 'CTRL'): 228,\n",
       " ('JYJ_CSF', 'MS'): 19,\n",
       " ('KJS_CSF', 'MS'): 171,\n",
       " ('YYW_CSF', 'MS'): 45}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sc.read_h5ad(f\"{dataset_name}_datasetDeclustered.h5ad\")\n",
    "\n",
    "current = pd.DataFrame(dataset.X, columns= dataset.var_names)\n",
    "current.insert(0, \"SampleID\", sampleID.values)\n",
    "current.insert(1, \"Label\", indicator.values)\n",
    "\n",
    "grouped = current.groupby(['SampleID', 'Label']).size()\n",
    "result_dict = {k:v for k,v in grouped.to_dict().items() if v != 0}\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5218283-70de-4f0a-9b3c-092b984cdf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['74594', '41540', 'KJS_CSF', '45044', '58637', '19270', 'JYJ_CSF', '32190', '60249'] ['83775', '49131', '71658', 'YYW_CSF']\n",
      "Dataset di train:\n",
      "(1712, 15653)\n",
      "I malati sono:  952\n",
      "I sani sono:  760\n",
      "\n",
      "Dataset di test:\n",
      "(431, 15653)\n",
      "I malati sono:  203\n",
      "I sani sono:  228\n",
      "\n",
      "79.88800746616892 20.111992533831078\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = ad.read_h5ad(f\"{dataset_name}_datasetDeclustered.h5ad\")\n",
    "current = pd.DataFrame(dataset.X, columns= dataset.var_names)\n",
    "current.insert(0, \"SampleID\", dataset.obs['sample'].values)\n",
    "current.insert(1, \"Label\", dataset.obs['condition'].values)\n",
    "\n",
    "test_ids = ['83775', '49131', '71658', 'YYW_CSF']\n",
    "train_ids = list(set(current['SampleID'].unique()) - set(test_ids))\n",
    "\n",
    "print(train_ids, test_ids)\n",
    "\n",
    "train = current[current['SampleID'].isin(train_ids)].sample(frac=1, random_state=42)\n",
    "test = current[current['SampleID'].isin(test_ids)].sample(frac=1, random_state=42)\n",
    "\n",
    "# Split the dataset using GroupShuffleSplit\n",
    "# splitter = GroupShuffleSplit(n_splits=2, test_size=0.25, random_state=523)\n",
    "# split = splitter.split(dataset.X, groups=dataset.obs['sample'])\n",
    "# train_inds, test_inds = next(split)\n",
    "\n",
    "# # Create AnnData subsets for training and testing\n",
    "# train = dataset[train_inds].copy()\n",
    "# test = dataset[test_inds].copy()\n",
    "\n",
    "print(\"Dataset di train:\")\n",
    "print(train.shape)\n",
    "print(\"I malati sono: \", sum(train['Label'] == 'MS'))\n",
    "print(\"I sani sono: \", sum(train['Label'] == 'CTRL'))\n",
    "\n",
    "print(\"\\nDataset di test:\")\n",
    "print(test.shape)\n",
    "print(\"I malati sono: \", sum(test['Label'] == 'MS'))\n",
    "print(\"I sani sono: \", sum(test['Label'] == 'CTRL'))\n",
    "\n",
    "y_train = train['Label'] == \"MS\"\n",
    "x_train = train.drop(columns=['SampleID', 'Label'])\n",
    "\n",
    "y_test = test['Label'] == \"MS\"\n",
    "x_test = test.drop(columns=['SampleID', 'Label'])\n",
    "\n",
    "sampleSum = dataset.X.shape[0]\n",
    "\n",
    "print()\n",
    "print(len(y_train)*100/sampleSum, len(y_test)*100/sampleSum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62561c-72b9-45f9-96dc-91e866314573",
   "metadata": {},
   "source": [
    "<h1>Addestramento modello</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85cfb0f7-6919-4562-884d-0aa32ab1effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyPrint(model, name, test=False):\n",
    "    print(name + \":\")\n",
    "    print(\"Iperparametri: \", model.best_params_)\n",
    "    print(\"Train f1: \", model.score(x_train, y_train))\n",
    "    print(\"Mean f1 cross-validated: \", model.best_score_)\n",
    "    best_index = model.best_index_\n",
    "    print(\"\\t\\t precision \\t\\t recall \\t\\t f1-score\")\n",
    "    print(f\"0 \\t\\t {model.cv_results_['mean_test_precision 0'][best_index]:.2f} \\t\\t\\t {model.cv_results_['mean_test_recall 0'][best_index]:.2f}\") \n",
    "    print(f\"1 \\t\\t {model.cv_results_['mean_test_precision 1'][best_index]:.2f} \\t\\t\\t {model.cv_results_['mean_test_recall 1'][best_index]:.2f}\")\n",
    "    print(f\"Accuracy \\t\\t\\t\\t\\t\\t\\t {model.cv_results_['mean_test_Accuracy'][best_index]:.2f}\")\n",
    "    print(f\"macro avg \\t {(model.cv_results_['mean_test_precision 0'][best_index] + model.cv_results_['mean_test_precision 1'][best_index]) / 2:.2f} \\t\\t\\t {(model.cv_results_['mean_test_recall 0'][best_index] + model.cv_results_['mean_test_recall 1'][best_index])/2:.2f} \\t\\t\\t {model.cv_results_['mean_test_f1'][best_index]:.2f}\")\n",
    "    if test:  \n",
    "        print(\"\\nTest f1 score: \", model.score(x_test, y_test))\n",
    "        print(classification_report(y_test, model.predict(x_test)), \"\\n\\n\")\n",
    "\n",
    "def bayesianOpt(pipeline, hyperparameters, iteration, scorer, njobs, x_train, y_train):\n",
    "    bayesianSearchResult = BayesSearchCV(estimator = pipeline, search_spaces=hyperparameters, cv=5, n_iter=iteration, return_train_score=True,  refit='f1', scoring=scorer, n_jobs=njobs, verbose=2)\n",
    "    bayesianSearchResult.fit(x_train, y_train)\n",
    "    print(\"Iperparametri:\", bayesianSearchResult.best_params_)\n",
    "    return bayesianSearchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7017ed5c-8c2c-400f-b1a6-774b616ff7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('scaling', MinMaxScaler()),\n",
    "    ('classifier', XGBClassifier(random_state=42))])\n",
    "\n",
    "def precision_class_0(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None)[0]\n",
    "\n",
    "def precision_class_1(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None)[1]\n",
    "\n",
    "def recall_class_0(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[0]\n",
    "\n",
    "def recall_class_1(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None)[1]\n",
    "\n",
    "scorer = {\n",
    "    'Accuracy': 'accuracy',\n",
    "    'precision 0': make_scorer(precision_class_0),\n",
    "    'precision 1': make_scorer(precision_class_1),\n",
    "    'recall 0': make_scorer(recall_class_0),\n",
    "    'recall 1': make_scorer(recall_class_1),\n",
    "    'f1': make_scorer(f1_score, average='macro')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfba282-871a-46dd-85de-ee82a4a01683",
   "metadata": {},
   "source": [
    "<h3>Bayesian Hyperparameter Optimization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7799db5-6d27-4cac-ad52-6ebdc0e4860e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'classifier__n_estimators': Integer(50, 600),  # Numero di alberi\n",
    "    'classifier__max_depth': Integer(2, 15),  # Profondità dell'albero\n",
    "    'classifier__learning_rate': Real(0.001, 0.7, prior='log-uniform'),  # Tasso di apprendimento\n",
    "    'classifier__gamma': Real(0.0001, 1, prior='log-uniform'),  # Minimum loss reduction\n",
    "    'classifier__min_child_weight': Integer(1, 8), \n",
    "    'classifier__scale_pos_weight': Categorical([1, 760/952]),\n",
    "    'classifier__reg_alpha': Real(0.0001, 100, prior='log-uniform'),  # Regolarizzazione L1\n",
    "    'classifier__reg_lambda': Real(0.0001, 100, prior='log-uniform')  # Regolarizzazione L2\n",
    "}\n",
    "\n",
    "bayesianOptResult = bayesianOpt(pipeline, param_dist, 600, scorer, 10, x_train, y_train)\n",
    "joblib.dump(bayesianOptResult, \"model1_CSF_B.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58eeaef-5db4-4f3f-b2c5-17187db5dd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Hyperparameter:\n",
      "Iperparametri:  OrderedDict([('classifier__gamma', 0.0001807075116973858), ('classifier__learning_rate', 0.31167512125611246), ('classifier__max_depth', 12), ('classifier__min_child_weight', 2), ('classifier__n_estimators', 182), ('classifier__reg_alpha', 0.44024813586192624), ('classifier__reg_lambda', 3.0385010957535226), ('classifier__scale_pos_weight', 0.7983193277310925)])\n",
      "Train f1:  1.0\n",
      "Mean f1 cross-validated:  0.9709521234025283\n",
      "\t\t precision \t\t recall \t\t f1-score\n",
      "0 \t\t 0.98 \t\t\t 0.96\n",
      "1 \t\t 0.97 \t\t\t 0.98\n",
      "Accuracy \t\t\t\t\t\t\t 0.97\n",
      "macro avg \t 0.97 \t\t\t 0.97 \t\t\t 0.97\n",
      "\n",
      "Test f1 score:  0.8719745643372645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.96      0.89       228\n",
      "        True       0.95      0.77      0.85       203\n",
      "\n",
      "    accuracy                           0.87       431\n",
      "   macro avg       0.89      0.87      0.87       431\n",
      "weighted avg       0.89      0.87      0.87       431\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bayesianOptResult = joblib.load(\"model1_CSF_B.pkl\")\n",
    "prettyPrint(bayesianOptResult, \"Bayesian Hyperparameter\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec015a-ddc9-41ce-9d87-cfb8f66672d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
